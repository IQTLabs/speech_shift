{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import aggregators\n",
    "import embeddors\n",
    "import data_utils\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, InputLayer\n",
    "from alibi_detect.cd import MMDDrift\n",
    "from alibi_detect.cd.preprocess import UAE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this component to the root of the VOiCES dataset\n",
    "DATASET_ROOT = '/Users/jberkowitz/Datasets/VOiCES_devkit'\n",
    "# Convenience function to add root to data path\n",
    "add_root = lambda x: os.path.join(DATASET_ROOT,x)"
   ]
  },
  {
   "source": [
    "First we will load the dataframe for the `training` slice of VOiCES, and perform a few cleaning operations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   index  chapter  degrees distractor  \\\n",
       "0      0     9960       60       musi   \n",
       "1      1     9960       60       musi   \n",
       "\n",
       "                                            filename gender  mic  \\\n",
       "0  distant-16k/speech/train/rm1/musi/sp0083/Lab41...      F    1   \n",
       "1  distant-16k/speech/train/rm1/musi/sp0083/Lab41...      F    5   \n",
       "\n",
       "                                          query_name room  segment  \\\n",
       "0  Lab41-SRI-VOiCES-rm1-musi-sp0083-ch009960-sg00...  rm1       42   \n",
       "1  Lab41-SRI-VOiCES-rm1-musi-sp0083-ch009960-sg00...  rm1       42   \n",
       "\n",
       "                                              source  speaker  \\\n",
       "0  source-16k/train/sp0083/Lab41-SRI-VOiCES-src-s...       83   \n",
       "1  source-16k/train/sp0083/Lab41-SRI-VOiCES-src-s...       83   \n",
       "\n",
       "                                          transcript  noisy_length  noisy_sr  \\\n",
       "0  the horrible glowing tool disappeared into the...        258880     16000   \n",
       "1  the horrible glowing tool disappeared into the...        258880     16000   \n",
       "\n",
       "   noisy_time  source_length  source_sr  source_time  \n",
       "0       16.18         258880      16000        16.18  \n",
       "1       16.18         258880      16000        16.18  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>chapter</th>\n      <th>degrees</th>\n      <th>distractor</th>\n      <th>filename</th>\n      <th>gender</th>\n      <th>mic</th>\n      <th>query_name</th>\n      <th>room</th>\n      <th>segment</th>\n      <th>source</th>\n      <th>speaker</th>\n      <th>transcript</th>\n      <th>noisy_length</th>\n      <th>noisy_sr</th>\n      <th>noisy_time</th>\n      <th>source_length</th>\n      <th>source_sr</th>\n      <th>source_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>9960</td>\n      <td>60</td>\n      <td>musi</td>\n      <td>distant-16k/speech/train/rm1/musi/sp0083/Lab41...</td>\n      <td>F</td>\n      <td>1</td>\n      <td>Lab41-SRI-VOiCES-rm1-musi-sp0083-ch009960-sg00...</td>\n      <td>rm1</td>\n      <td>42</td>\n      <td>source-16k/train/sp0083/Lab41-SRI-VOiCES-src-s...</td>\n      <td>83</td>\n      <td>the horrible glowing tool disappeared into the...</td>\n      <td>258880</td>\n      <td>16000</td>\n      <td>16.18</td>\n      <td>258880</td>\n      <td>16000</td>\n      <td>16.18</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>9960</td>\n      <td>60</td>\n      <td>musi</td>\n      <td>distant-16k/speech/train/rm1/musi/sp0083/Lab41...</td>\n      <td>F</td>\n      <td>5</td>\n      <td>Lab41-SRI-VOiCES-rm1-musi-sp0083-ch009960-sg00...</td>\n      <td>rm1</td>\n      <td>42</td>\n      <td>source-16k/train/sp0083/Lab41-SRI-VOiCES-src-s...</td>\n      <td>83</td>\n      <td>the horrible glowing tool disappeared into the...</td>\n      <td>258880</td>\n      <td>16000</td>\n      <td>16.18</td>\n      <td>258880</td>\n      <td>16000</td>\n      <td>16.18</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "full_index_df = pd.read_csv(add_root('references/train_index.csv'))\n",
    "# Drop recordings that don't match in length to source audio\n",
    "trimmed_index = full_index_df[full_index_df['noisy_length']==full_index_df['source_length']]\n",
    "trimmed_index.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "3200"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "len(trimmed_index[trimmed_index['distractor']=='musi'])"
   ]
  },
  {
   "source": [
    "Next we initialize the embeddors"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2020-12-06 13:23:05 | INFO | fairseq.models.wav2vec.wav2vec | Wav2VecModel(\n  (feature_extractor): ConvFeatureExtractionModel(\n    (conv_layers): ModuleList(\n      (0): Sequential(\n        (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n        (3): ReLU()\n      )\n      (1): Sequential(\n        (0): Conv1d(512, 512, kernel_size=(8,), stride=(4,), bias=False)\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n        (3): ReLU()\n      )\n      (2): Sequential(\n        (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), bias=False)\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n        (3): ReLU()\n      )\n      (3): Sequential(\n        (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), bias=False)\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n        (3): ReLU()\n      )\n      (4): Sequential(\n        (0): Conv1d(512, 512, kernel_size=(4,), stride=(2,), bias=False)\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n        (3): ReLU()\n      )\n      (5): Sequential(\n        (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n        (3): ReLU()\n      )\n      (6): Sequential(\n        (0): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n        (1): Dropout(p=0.0, inplace=False)\n        (2): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n        (3): ReLU()\n      )\n    )\n  )\n  (feature_aggregator): ConvAggegator(\n    (conv_layers): Sequential(\n      (0): Sequential(\n        (0): ReplicationPad1d((1, 0))\n        (1): Conv1d(512, 512, kernel_size=(2,), stride=(1,))\n        (2): Dropout(p=0.0, inplace=False)\n        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n        (4): ReLU()\n      )\n      (1): Sequential(\n        (0): ReplicationPad1d((2, 0))\n        (1): Conv1d(512, 512, kernel_size=(3,), stride=(1,))\n        (2): Dropout(p=0.0, inplace=False)\n        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n        (4): ReLU()\n      )\n      (2): Sequential(\n        (0): ReplicationPad1d((3, 0))\n        (1): Conv1d(512, 512, kernel_size=(4,), stride=(1,))\n        (2): Dropout(p=0.0, inplace=False)\n        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n        (4): ReLU()\n      )\n      (3): Sequential(\n        (0): ReplicationPad1d((4, 0))\n        (1): Conv1d(512, 512, kernel_size=(5,), stride=(1,))\n        (2): Dropout(p=0.0, inplace=False)\n        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n        (4): ReLU()\n      )\n      (4): Sequential(\n        (0): ReplicationPad1d((5, 0))\n        (1): Conv1d(512, 512, kernel_size=(6,), stride=(1,))\n        (2): Dropout(p=0.0, inplace=False)\n        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n        (4): ReLU()\n      )\n      (5): Sequential(\n        (0): ReplicationPad1d((6, 0))\n        (1): Conv1d(512, 512, kernel_size=(7,), stride=(1,))\n        (2): Dropout(p=0.0, inplace=False)\n        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n        (4): ReLU()\n      )\n      (6): Sequential(\n        (0): ReplicationPad1d((7, 0))\n        (1): Conv1d(512, 512, kernel_size=(8,), stride=(1,))\n        (2): Dropout(p=0.0, inplace=False)\n        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n        (4): ReLU()\n      )\n      (7): Sequential(\n        (0): ReplicationPad1d((8, 0))\n        (1): Conv1d(512, 512, kernel_size=(9,), stride=(1,))\n        (2): Dropout(p=0.0, inplace=False)\n        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n        (4): ReLU()\n      )\n      (8): Sequential(\n        (0): ReplicationPad1d((9, 0))\n        (1): Conv1d(512, 512, kernel_size=(10,), stride=(1,))\n        (2): Dropout(p=0.0, inplace=False)\n        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n        (4): ReLU()\n      )\n      (9): Sequential(\n        (0): ReplicationPad1d((10, 0))\n        (1): Conv1d(512, 512, kernel_size=(11,), stride=(1,))\n        (2): Dropout(p=0.0, inplace=False)\n        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n        (4): ReLU()\n      )\n      (10): Sequential(\n        (0): ReplicationPad1d((11, 0))\n        (1): Conv1d(512, 512, kernel_size=(12,), stride=(1,))\n        (2): Dropout(p=0.0, inplace=False)\n        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n        (4): ReLU()\n      )\n      (11): Sequential(\n        (0): ReplicationPad1d((12, 0))\n        (1): Conv1d(512, 512, kernel_size=(13,), stride=(1,))\n        (2): Dropout(p=0.0, inplace=False)\n        (3): Fp32GroupNorm(1, 512, eps=1e-05, affine=True)\n        (4): ReLU()\n      )\n    )\n    (residual_proj): ModuleList(\n      (0): None\n      (1): None\n      (2): None\n      (3): None\n      (4): None\n      (5): None\n      (6): None\n      (7): None\n      (8): None\n      (9): None\n      (10): None\n      (11): None\n    )\n  )\n  (wav2vec_predictions): Wav2VecPredictionsModel(\n    (project_to_steps): ConvTranspose2d(512, 512, kernel_size=(1, 12), stride=(1, 1))\n    (dropout): Dropout(p=0.0, inplace=False)\n  )\n  (dropout_feats): Dropout(p=0.0, inplace=False)\n  (dropout_agg): Dropout(p=0.0, inplace=False)\n)\n"
     ]
    }
   ],
   "source": [
    "w2v = embeddors.Wav2VecEmbeddor(weight_path='models/wav2vec_large.pt')\n",
    "trill=embeddors.TrillEmbeddor()"
   ]
  },
  {
   "source": [
    "Initialize the aggregators"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ag = aggregators.MeanAggregator()\n",
    "pca_ag = aggregators.PCAAggregator()"
   ]
  },
  {
   "source": [
    "We will also use untrained autoencoders (UAE) to do some preliminary dimension reduction, as in [Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift](https://arxiv.org/pdf/1810.11953.pdf).  We will initialize a UAE for both Trill and Wave2Vec embeddings, as they have different dimensionality. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(0) # set seed for reproducibility\n",
    "encoding_dim=10 # fixed dimensionality\n",
    "\n",
    "w2v_encoder_net = tf.keras.Sequential(\n",
    "    [\n",
    "        InputLayer(input_shape=(512,)),\n",
    "        Dense(100,),\n",
    "        Dense(encoding_dim,)\n",
    "    ]\n",
    ")\n",
    "w2v_uae = UAE(w2v_encoder_net)\n",
    "\n",
    "trill_encoder_net = tf.keras.Sequential(\n",
    "    [\n",
    "        InputLayer(input_shape=(2048,)),\n",
    "        Dense(100,),\n",
    "        Dense(encoding_dim,)\n",
    "    ]\n",
    ")\n",
    "trill_uae = UAE(trill_encoder_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}